{% extends 'index.html' %}

{% block content %}
    {% for post in posts %}
        {% %}
    {% endfor %}
{% endblock %}





<div class="note">
                <div class="note-title">
                    <h3>
                        Deploying Production React app 
                    </h3>
                    <small>
                        Cause React is entrypoint for entry level frontend developers,
                        and it's a popular framework 
                    </small>
                </div>
                <div class="note-body">
                    <p>
                        So, is time to deploy production. You ask frontend team
                        (always arrogant) lead: why deploy instructions dictate only how to run react
                        development build? Where production instructions?
                        <span><img height=15 src="./static/images/gifs/confused.gif" /></span>
                    </p>
                    <p>
                        But when I deploy and see a source tree map of the entire
                        application on the browser development settings, with all the
                        source code for the frontend.
                    </p>
                    <p>
                        Ask frontend for solution to do this, they say "not sure".
                    </p>
                    <p>
                        When deploying for production react apps, react is nice
                        enough to let you compile the app in a single build/
                        directory. For not generating source map, a small but *very*
                        important step.
                    </p>
                    <p>
                        Dockerfile for nginx/frontend
                    </p>
                    <div class="pre-code"><code>
#syntax=docker/dockerfile:1
FROM node AS frontend
WORKDIR /frontend
COPY frontend .
RUN yarn install
RUN GENERATE_SOURCEMAP=false npm run build

FROM nginx

RUN rm -rfvd /etc/nginx/conf.d && \
    mkdir -p /usr/share/nginx/html/static && \
    mkdir -p /var/www/html && \
    mkdir -p /etc/nginx/conf.d

COPY --from=frontend /frontend/build /var/www/html
COPY ./docker/nginx/conf.d/default.conf /etc/nginx/conf.d/default.conf</code>
                    </div>
                    <p>
                    The main thing to look at is the RUN step for npm run build;
                    passing environment variable behind it,
                    GENERATE_SOURCEMAP=false does the trick. Multi-stage builds used
                    on the Dockerfile, check docker best practices for advices.
                    <br><br>
                    Seems like small step, but makes big difference; see figure
                    1.2 below
                    </p>
                    <p>
                        No source map tree on production builds, sysadmin can rest
                        better.
                    </p>
                    <div style="width: auto; overflow: auto;">
                        <figure style="margin: 0;">
                            <img src="./static/images/images/step-one.png">
                            <figcaption>Figure 1.1</figcaption>
                        </figure>
                        <br>
                        <figure style="margin: 0;">
                            <img width=auto src="./static/images/images/step-two.png">
                            <figcaption>Figure 1.2</figcaption>
                        </figure>
                    </div>
                    <p>
                        Last thing to do is modify nginx config file so that it can
                        search for that directory, which was copied to /var/www/html
                        inside nginx container (see nginx build stage on Dockerfile)
                    </p>
                    <div class="pre-code"><code>
upstream backend {
    server api:6969 weight=2;
}

server {
    ...
    ...

    location /api/ {
        ...
        ...
        proxy_pass http://backend;
    }

    location / {
        root /var/www/html;
        try_files $uri /index.html;
    }

}</code>
                    </div>
                </div>
            </div>
            <div class="note" >
                <div class="note-title">
                    <h3>Celery Flower and Worker</h3>
                </div>
                <div class="note-body">
                    <p>
                    Sometimes deploying systems with asynchronous tasks is
                    required per client specifications.
                    </p>
                    <p>
                    For these instances I tend to dockerize celery redis and
                    monitor them with Flower, which provides visualization for
                    the performance of system asychronous tasks.
                    <div class="pre-code"><code>
version: '3.9'

services:
  .... rest of services (redis, etc)
  tasks:
    build:
      context: .
      dockerfile: ./docker/tasks/Dockerfile
    command: >
      sh -c "python3 -m celery -A my_system flower --address=0.0.0.0 --port=5566 &; python3 -m celery -A my_system worker -l INFO -E"
    env_file:
      - ./docker/tasks/.env
    depends_on:
      - redis
    restart: unless-stopped
    network:
    my-network:
        ipv4_address: 10.10.10.2
    expose:
      - "5566"

networks:
  ... network config</code>
                    </div>
                    <small>
                        On the command: section of the dockerfile, putting flower
                        on the background so the worker can operate normally, notice
                        the ampersand <code> & </code> and the <code> ; </code> if
                        the previous command fails, the container won't be marked as
                        <code>exited</code> by the docker daemon.
                    </small>
                    <p>
                        Usually setting up a shared task monitoring tool over public
                        http is not the best idea.

                        I pipe to my remote workstation machine all flower instances for all
                        systems using ssh
                    </p>
                    <div class="pre-code"><code>
~$ ssh -L 10.10.10.2:5566:127.0.0.1:8001 -i my_system.pem user@$MY_SYSTEM_IP</code>
                    </div>
                    <p>
                    Open browser at <code> 127.0.0.1:8001 </code> and monitor my
                    systems shared tasks, configure rate limits, etc ....
                    <br>
                    You can fine tune flower, check out their documentation
                    </p>
                </div>
            </div>
            <div class="note" >
                <div class="note-title">
                    <h3> Decided to add some vendor libs </h3>
                </div>
                <div class="note-body">
                    <p> Have been a little against loading vendor libraries to my site,
                    however, needed some syntax higlighter and added
                    <a href="https://highlightjs.org" target="_blank">highlight.js</a>
                     for better clarity.
                    </p>
                </div>
            </div>
            <div class="note">
                <div class="note-title">
                    <h3>OCI Review<h3>
                </div>
                <div class="note-body">
                    <p> Open Systems Interconnection Reference Model</p>
                    <small> FOr people getting into INFOSEC, this is crucial </small>

                    <p> 
                        It is a reference for how a network protocol stack should operate.
                        Each layer provides a set of function to communicate with the
                        layer above, and it also relies on sets of function from the
                        layer below ... Vertical Communication. 
                    </p>
                    <h3>The 7 layers</h3>
                    <ol>
                        <li>
                            Application - File Transfers, Emulators, Restful APIS
                        </li>
                        <li>
                            Presentation - Formating of information or encryption
                        </li>
                        <li>
                            Session - Keep Alive Sessions
                        </li>
                        <li>
                            Transport - Reliable END 2 END transport (TCP/IP UDP Protocols, etc)
                        </li>
                        <li>
                            Network - Packet deliver, routing.
                        </li>
                        <li>
                            Data Link - Framing of information units and error
                            handling.
                        </li>
                        <li>
                            Physical - Transmission of bits on the actual hardware.
                        </li>
                    </ol>
                    <p>
                        <bold>Most</bold> applications regular users of the internet use are at
                        the Application, Presentation, and Session levels of the OCI Model.
                    </p>
                    <small> Source: IBM TCP/IP Technical Overview, Sixth Edition</small>
                </div>
            </div>
            <div class="note">
                <div class="note-title">
                    <h3> % digits deployments</h3>
                </div>
                <div class="note-body">
                    <p>
                        Deploying stuff on servers, could be just relayer api's for remote
                        RPC's on complex system. 
                    </p>
                    <p>
                        If the server is not hosted on a cloud provider, then a simple
                        IPTABLES execution will do it, given the server is LInuX, can be
                        house computer, but need to modify  your router to redirect incomming
                        traffic to port whatever to machine serving IP, that machine has some
                        reverse proxy sitting infront of it.
                        <br>
                    <p> open up some ports </p>
                    <div class="pre-code"><code>
iptables -I INPUT -p tcp -m tcp --dport 80 -j ACCEPT
iptables -I INPUT -p tcp -m tcp --dport 443 -j ACCEPT</code>
                    </div>
                    </p>
                    <p> 
                        For now docker-compose example for serving a static site w nginx,
                        pretty stupid and noob, the spin are the quirks for hardening the web
                        server and scoring over a D on <code> securityheaders.org </code>,
                        and proxy log analysis for monitoring. Remoteley smart competition
                        can eat your bread by presenting a report with your application list
                        of vulnerabilities, because not many devs are security oriented, they
                        just want to turn in a job and get paid.
                    <p>
                        <bold> 
                            A compose file for the host (install docker & docker-compose)
                        </bold>
<div class="pre-code"><code>
version: '3.9'
services:
    web-server:
    build:
        context: .
        dockerfile: nginx/Dockerfile
    ports:
      - "80:80"
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
    restart: unless-stopped
    networks:
      my-network:
        ipv4_address: 10.10.10.7

networks:
    my-network:
      ipam:
        driver: default
        config:
          - subnet: 10.10.10.0/24
</code></div>
                        <small> 
                        Enabling volumes to avoid rebuilding app, and editing directly on
                        host, not on container, saves the sysadmin time. 
                        </small>
                    <p> Dockerfile located on build context nginx/Dockerfile </p>
 <div class="pre-code"><code>
# syntax=docker/dockerfile:1
FROM nginx
RUN rm /etc/nginx/conf.d
RUN mkdir -p /var/www/html
COPY nginx/conf.d /etc/nginx/conf.d
COPY website /var/www/html
RUN ls /var/www/html; echo "[!] Build finished"    
 </code></div>
                    <p> Nginx conf file </p>
<div class="pre-code"><code>
server {
    listen 80;
    add_header X-Frame-Options SAMEORIGIN;
    add_header X-Content-Type-Options nosniff;
    root /var/www/html;
    location / {
        try_files $uri /index.html =404;
    }
}
</code></div>
                    <p> 
                        <br>
                        However, I am now interested in monitoring visitors and
                        keeping track of IP's by parsing my reverse proxy logs, I don't wanna use
                        something like Dynatrace or New Relic, because I am interested in using
                        something like OpenCTI to monitor APT's on my stupid website.
                        <br><br>
                    <div class="pre-code"><code>
# dump logs 2 file
~$ docker-compose logs --follow web-server &> /var/log/web-server.log &</code>
                    </div>
                    <small> Start listening to container logs on the background, could be
                    done as a systemctl unit file, but this serves the purpose,
                    monitor this process and kill it with htop or the <code>kill</code> cmd
                    </small>
                    <br>
                    <br>
                    <p>
                        For the purpose a super simple shell or python script to parse
                        this log file works, only want something fairly stupid, which is to extract ip, with
                        datetime from the logfile, so to ship it to any analysis
                        program or system -- for further analysis, in this case further
                        processing for analysis and visualization.
                    </p>
                    <div class="pre-code"><code>
#!/bin/bash
grep -E -o "([0-9]{1,3}[\.]){3}[0-9]{1,3}\s-\s-\s\[[0-9]{1,2}\/\w+\/[0-9]{4}\:[0-9]{2}\:[0-9]{2}\:[0-9]{2}.*\]" /var/log/web-server.log > /usr/share/data/monitoring/web-server-visitors.txt</code>
                    </div>
                    <p>
                    <div style="width: auto; height: 350px;overflow: auto;">
                        <img src="./static/images/gifs/ips.gif" width=auto class="console-div"/>
                    </div>
                    <small> 
                    <code> 
                    testing the code above
                    </code>
                    </small>
                    <br>
                    <br>
                    The date can serve as a distinct reference point for entries on
                    whatever storage unit u wish to persist analysis.
                    <br><br>
                    The last step is to cook a script to load this file, and either
                    send it as a batch to another server on the same network, or,
                    another network running some api that receives this data and
                    analyses with a forensics tool like OpenCTI (open source).
                    <br>
                    <br>
                    <div class="pre-code"><code>
#!/bin/bash
curl -d @/usr/share/data/monitoring/web-server-visitors.txt http://10.0.10.2/analyze-web-server-visistors</code>
                    </div>
                    <small>
                    Either add this script with 
                    <code>
                    crontab -e
                    </code> or add it to 
                    <code> 
                    /etc/cron.hourly 
                    </code>
                    </small>
                    <br>
                    </p>
                    </p>
                </div>
            </div>
            <div class="note">
                <div class="note-title">
                    <h3>  management, systems, django .. BORING </h3>
                </div>
                <div class="note-body">
                    <p> 
                        Sometimes u just don't want to add 3rd party packages to handle crons
                        for backend web api's that run on python and django, so here is a STUPID
                        way of doing it. Add stuff to 
                        <code>/etc/cron.daily</code> 
                        or 
                        <code> /etc/cron.hourly </code>
                    </p>
<div class="pre-code"><code>
#!/path/to/venv/bin/python
# Django code maybe cron.hourly or 
# cron.daily tasks that require r/w db ops.

import os
import random
import sys

# Used to import needed defined modules
sys.path.append("/home/user/project/base_dir/") 

# NeeDed By Django
settings = "xxx.settings"
os.environ["DJANGO_SETTINGS_MODULE"] = settings

import django
django.setup()

# Defin3d crons
from xxx.crons import rotate_proxy

if __name__ == "__main__":
    rotate_proxy()
</code></div>
                </div>
            </div>
        </div>
        <!-- end of notes -->

